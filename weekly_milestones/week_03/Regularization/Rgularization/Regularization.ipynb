{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc87ee5",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4219db",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a412b0",
   "metadata": {},
   "source": [
    "### 1.1 What is regularization?\n",
    "\n",
    "* Regularization is a set of techniques that constrain the complexity of models. They thus reduce the risk of overfitting and improve the ability of a model to generalize. in other words, We can use new models that are almost like Linear Regression but with a different loss function.\n",
    "\n",
    "**GOAL OF USING REGULARIZATION MODELS**: Reduce Overtitting; and make our model more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048c1b2",
   "metadata": {},
   "source": [
    "### 1.2 When to regularize:\n",
    "* To reduce overfitting - check if you are overfitting via the usual methods:\n",
    "    * cross validation\n",
    "    * train/validation score differing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a0198",
   "metadata": {},
   "source": [
    "### 1.3 How to regularize:\n",
    "* Perform the usual ML workflow, \n",
    "* Be sure to normalize your dataset before fitting (`sklearn.preprocessing.StandardScaler`)\n",
    "* Now use a **regularization** model instead of a normal linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7fc5e",
   "metadata": {},
   "source": [
    "## 2. The maths "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f4ea1",
   "metadata": {},
   "source": [
    "### 2.1 LOSS FUNCTION \n",
    "#### Residual Sum of Squares (RSS) is just Mean Squared Error without the Mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b66d47",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y})^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^N (y_i - \\hat{y})^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8506a25",
   "metadata": {},
   "source": [
    "#### Which we can rewrite by substituting the linear regression equation in for yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabad3d5",
   "metadata": {},
   "source": [
    "$ \\hat{y} = w_0 + [w_1x_1 + ... + w_nx_n] $ \n",
    "\n",
    "$ m = slope, w_0 = intercept $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0cb0d6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = (w_0 + \\sum_{j=1}^M w_j x_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^N (y_i - (w_0 + \\sum_{j=1}^M w_j x_{j}))^2 \n",
    "$$\n",
    "\n",
    "*`j=1 -> M` - no. of features (cols in the dataframe)*\n",
    "\n",
    "*`i=1 -> N` - no. of data points (rows in the dataframe)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05b061",
   "metadata": {},
   "source": [
    "## Regularisation means adding an extra penalty to RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafa186",
   "metadata": {},
   "source": [
    "### 2.2 Ridge regression \n",
    "\n",
    "\n",
    "* Add a penalising term that shrinks the **square of the weights**\n",
    "\n",
    "* Controled by a regularising term `alpha`\n",
    "\n",
    "* `alpha` is a **hyperparameter** that we set when we instantiate the model - if alpha is zero, Ridge becomes vanilla Linear Regression\n",
    "\n",
    "* large loss (big outliers in some feature) in a feature that is useful as predictor (large weight) create MASSIVE Ridge losses \n",
    "\n",
    "* How does Ridge handle this - reduces the coefficient for that X feature to a low number\n",
    "\n",
    "* Ridge is also called `L2` regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21655599",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "Ridge = \\sum_{i=1}^N (y_i - (w_0 + \\sum_{j=1}^M w_j x_{j}))^2  + \\sum_{j=1}^M \\alpha w_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e93be3",
   "metadata": {},
   "source": [
    "### 2.3 Lasso regression \n",
    "\n",
    "* Add a penalising term that shrinks the **absolute value of the weights**\n",
    "\n",
    "* Controled by a regularising term `alpha`\n",
    "\n",
    "* Tends to result in the coefficients for many features becoming zero (in ridge they become close to zero, but tend not to be zero)\n",
    "\n",
    "* Lasso is also called `L1` regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdd07b",
   "metadata": {},
   "source": [
    "$$\n",
    "Lasso = RSS + \\sum_{i=1}^M \\alpha \\vert w_i \\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326554a0",
   "metadata": {},
   "source": [
    "### 2.4 Mix Lasso and Ridge with `ElasticNet`\n",
    "* Combine L1 (Lasso) and Ridge (L2) by setting the `l1_ratio` \n",
    "* l1_ratio = lasso / lasso + ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1effb0",
   "metadata": {},
   "source": [
    "### Additional Reading\n",
    "\n",
    "[StatQuest on Lasso](https://www.youtube.com/watch?v=NGf0voTMlcs)  \n",
    "[StatQuest on Ridge](https://www.youtube.com/watch?v=Q81RR3yKn30)  \n",
    "[StatQuest on ElasticNet](https://www.youtube.com/watch?v=1dKRdX9bfIo)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61170fb6",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa09ed0",
   "metadata": {},
   "source": [
    "## 2. Lets implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77539fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and visualization stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning stack\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c36b6",
   "metadata": {},
   "source": [
    "Create data following $\\sqrt{x}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8679e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a random state\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data set fluctuating around squre root of x\n",
    "X=np.arange(1,60, 5) # from 1 to 60 in steps of 6\n",
    "y=[np.sqrt(xi)+np.random.normal(0, 0.5) for xi in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dace04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the X to a 2D array for later use in sklearn mdels\n",
    "X=X.reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da8b1c",
   "metadata": {},
   "source": [
    "### Underfitting (high bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear regression on the data\n",
    "model=LinearRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y by lr model\n",
    "y_pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both linear regression line and the original data\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6eecb",
   "metadata": {},
   "source": [
    "### Underfitting:\n",
    "\n",
    "To see if you got an underfitting model, compare the scores:       \n",
    "`model.score(X_train, y_train)`    \n",
    "`model.score(X_test, y_test)`\n",
    "     \n",
    "If both scores are weak, you have probably an underfit situation\n",
    "     \n",
    "How could it happen?\n",
    " * Small data sets \n",
    " * Weak feature engineering\n",
    "     * Too little features\n",
    "     * Features uninformative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c08528",
   "metadata": {},
   "source": [
    "### Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75550b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pakages from sklearn for adding more terms to the equation\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the feature matrix to a polynomial form with the degree of 10\n",
    "poly=PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly=poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e192547",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the linear regression model on X_poly\n",
    "model=LinearRegression()\n",
    "model.fit(X_poly,y)\n",
    "y_pred_poly=model.predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fitted line and original data\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700563a8",
   "metadata": {},
   "source": [
    "### Overfitting: \n",
    "To see if you got an overfitting model, compare the scores:     \n",
    " `model.score(X_train, y_train)`    \n",
    " `model.score(X_test, y_test)`\n",
    "     \n",
    "If the train score is exceptionally good and the test score is weak, you probably have an overfit situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing new packages for lasso, ridge and elasticnet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming new models\n",
    "lasso=Lasso()\n",
    "ridge=Ridge()\n",
    "elast=ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a50c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting new models on X_poly\n",
    "lasso.fit(X_poly, y)\n",
    "ridge.fit(X_poly, y)\n",
    "elast.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202948f",
   "metadata": {},
   "source": [
    "If the model does not converge, the gradient did not reduce under the set tolerance during the set maximal iteration steps. This can happen easily with regularization, still you can try the following:     \n",
    "* increase `max_iter` (maybe some more steps help)\n",
    "* increase `tol` (being more generous could help)   \n",
    "\n",
    "Both measures should be taken carefully since it could increase the optimization time or make the results worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate y_pred by new models\n",
    "y_lasso=lasso.predict(X_poly)\n",
    "y_ridge=ridge.predict(X_poly)\n",
    "y_elast=elast.predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the models and comparing them\n",
    "plt.scatter(X,y, label='actual')\n",
    "#plt.plot(X, y_pred_poly, label='poly')\n",
    "#plt.plot(X, y_pred, label='Linearregression')\n",
    "#plt.plot(X, y_lasso, label='lasso')\n",
    "#plt.plot(X, y_ridge, label='ridge')\n",
    "#plt.plot(X, y_elast, label='elast')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67916d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e96f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a4b2b",
   "metadata": {},
   "source": [
    "### Additional Reading\n",
    "[Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be9af0",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409d5a1",
   "metadata": {},
   "source": [
    "## 3. Your Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb16018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the lasso and Ridge results by changing the hyperparameter\n",
    "lasso_1=Lasso(alpha=1) # 1, 10, 100, 1000\n",
    "ridge_1=Ridge(alpha=1) # 1, 10, 100, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6c8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
